{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044dee5-bdf2-4b6c-8285-8182f2823915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERAS introduction & implementation\n",
    "\"\"\"\n",
    "Keras'ta bir sinir ağı oluşturmanın çeşitli adımları vardır. Burada sırasıyla bu adımlardan ve adımlarla ilgili bazı olgulardan \n",
    "bahsedeceğiz.\n",
    "\n",
    "1) Öncelikle bir model nesnesi oluşturulmalıdır. tensorflow.keras modülü içerisinde çeşitli model sınıfları bulunmaktadır. \n",
    "En çok kullanılan model sınıfı Sequential isimli sınıftır. Tüm model sınıfları Model isimli sınıftan türetilmiştir. Sequential \n",
    "modelde ağa her eklenen katman sona eklenir. Böylece ağ katmanların sırasıyla eklenmesiyle oluşturulur. Sequential nesnesi \n",
    "yaratılırken name parametresiyle modele bir isim de verilebilir. Örneğin:\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential(name='Sample')\n",
    "\n",
    "Aslında Sequential nesnesi yaratılırken katmanlar belirlendiyse layers parametresiyle bu katmanlar da verilebilmektedir. Ancak\n",
    "sınıfın tipik kullanımında katmanlar daha sonra izleyen maddelerde ele alınacağı gibi sırasıyla eklenmektedir.\n",
    "\n",
    "2) Model nesnesinin yaratılmasından sonra katman nesnelerinin oluşturulup model nesnesine eklenmesi gerekir. Keras'ta farklı \n",
    "gereksinimler için farklı katman sınıfları bulundurulmuştur. En çok kullanılan katman sınıfı tensorflow.keras.layers modülündeki \n",
    "Dense sınıfıdır. Dense bir katman modele eklendiğinde önceki katmandaki tüm nöronların çıktıları eklenen katmandaki nöronların \n",
    "hepsine girdi yapılmaktadır. Bu durumda örneğin önceki katmanda k tane nöron varsa biz de modele n tane nörondan oluşan bir Dense \n",
    "katman ekliyorsak bu durumda modele (k * n + n) tane yeni parametre (yani tahmin edilmesi gereken parametre) eklemiş oluruz. Burada \n",
    "k * n tane ayarlanması gereken w (ağırlık) değerleri ve n tane de ayarlanması gereken bias değerleri söz konusudur. Bir nörondaki \n",
    "w (ağırlık) değerlerinin o nörona giren nöron sayısı kadar olduğuna ve bias değerlerinin her nöron için bir tane olduğuna dikkat\n",
    "ediniz.\n",
    "\n",
    "Dense sınıfının __init__ metodunun ilk parametresi eklenecek katmandaki nöron sayısını belirtir. İkinci parametre olan activation \n",
    "parametresi o katmandaki tüm nöronların aktivasyon fonksiyonlarının ne olacağını belirtmektedir. Bu parametreye aktivasyon \n",
    "fonksiyonları birer yazı biçiminde isimsel olarak girilebilir. Ya da tensorflow.keras.activations modülündeki fonksiyonlar \n",
    "olarak girilebilir. Örneğin:\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "layer = Dense(100, activation='relu')\n",
    "\n",
    "ya da örneğin:\n",
    "\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "layer = Dense(100, activation=relu)\n",
    "\n",
    "Dense fonksiyonun use_bias parametresi default durumda True biçimdedir. Bu parametre katmandaki nöronlarda \"bias\" değerinin \n",
    "kullanılıp kullanılmayacağını belirtmektedir. Metodun kernel_initializer parametresi katmandaki nöronlarda kullanılan w \n",
    "parametrelerinin ilkdeğerlerinin rastgele biçimde hangi algoritmayla oluşturulacağını belirtmektedir. Bu parametrenin default\n",
    "değeri \"glorot_unfiorm\" biçimindedir. Metodun bias_initializer parametresi ise katmandaki nöronların \"bias\" değerlerinin başlangıçta \n",
    "nasıl alınacağını belirtmektedir. Bu parametrenin default değeri de \"zero\" biçimdedir. Yani bias değerleri başlangıçta 0 \n",
    "durumundadır. Dense sınıfının __init__ metodunun diğer parametreleri hakkında bu aşamada bilgi vermeyeceğiz. \n",
    "\n",
    "Keras'ta Sequential modelde girdi katmanı programcı tarafından yaratılmaz. İlk saklı katman yaratılırken girdi katmanındaki \n",
    "nöron sayısı input_dim parametresiyle ya da input_shape parametresiyle belirtilmektedir. input_dim tek boyutlu girdiler için \n",
    "input_shape ise çok boyutlu girdiler için kullanılmaktadır. Örneğin:\n",
    "\n",
    "layer = Dense(100, activation='relu', input_dim=8) #8 sutun sayisina denk geliyor demek\n",
    "\n",
    "Tabii input_dim ya da input_shape parametrelerini yalnızca ilk saklı katmanda kullanabiliriz. Genel olarak ağın girdi katmanında\n",
    "dataset_x'teki sütun sayısı kadar nöron olacağına göre ilk katmandaki input_dim parametresini aşağıdaki gibi de girebiliriz:\n",
    "\n",
    "layer = Dense(100, activation='relu', input_dim=training_dataset_x.shape[1])\n",
    "    \n",
    "Aslında Keras'ta girdi katmanı için tensorflow.keras.layers modülünde Input isminde bir karman da kullanılmaktadır. Tenseoflow'un\n",
    "yeni versiyonlarında girdi katmanının Input katmanı ile oluşturulması istenmektedir. Aksi takdirde bu yeni versiyonlar uyarı \n",
    "vermektedir. Girdi katmanını Input isimli katman sınıfıyla oluştururken bu Input sınıfının __init__ metodunun birinci parametresi\n",
    "bir demet biçiminde (yani sahpe olarak) girilmelidir. Örneğin:\n",
    "\n",
    "input = Input((8, ))\n",
    "\n",
    "Burada 8 nöronşuk bir girdi katmanı oluşturulmuştur. Yukarıda da belirttiğimiz gibi eskiden ilk saklı katmanda girdi katmanı \n",
    "belirtiliyordu. Ancak Tensorflow kütüphanesinin yeni verisyonlarında ilk saklı katmanda girdi katmanının belirtilmesi artık \n",
    "uyarıya (warning) yol açmaktadır. Önceki kurslarda biz girdi katmanını ilk saklı katmanda belirtiyorduk. Ancak artık bu kursumuzda\n",
    "girdi katmanını ayrı bir Input nesnesi ile oluşturacağız. \n",
    "\n",
    "Her katmana istersek name parametresi ile bir isim de verebiliriz. Bu isimler model özeti alınırken ya da katmanlara erişilirken \n",
    "kullanılabilmektedir. Örneğin:\n",
    "\n",
    "layer = Dense(100, activation='relu',  name='Hidden-1')\n",
    "\n",
    "Oluşturulan katman nesnesinin model nesnesine eklenmesi için Sequential sınıfının add metodu kullanılmaktadır. Örneğin:\n",
    "\n",
    "input = Input((8, ))\n",
    "model.add(input)\n",
    "layer = Dense(100, activation='relu',  name='Hidden-1')\n",
    "model.add(layer)\n",
    "\n",
    "Programcılar genellikle katman nesnesinin yaratılması ve eklenmesini tek satırda aşağıdaki gibi yaparlar:\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=9, name='Hidden-1'))\n",
    "\n",
    "3) Modele katmanlar eklendikten sonra bir özet bilgi yazdırılabilir. Bu işlem Sequential sınıfının summary isimli metoduyla \n",
    "yapılmaktadır.\n",
    "\n",
    "Yukarıda da belirttiğimiz gibi bir katmandaki \"eğitilebilir (trainable)\" parametrelerin sayısı şöyle hesaplanmaktadır: Önceki \n",
    "katmanın çıktısındaki nöron sayısı, bu katmana dense biçimde bağlandığına göre toplamda \"önceki katmandaki nöron sayısı * \n",
    "bu katmandaki nöron sayısı\" kadar w parametresi ayarlanacaktır. Öte yandan bu katmandaki nöronların her birinde bir \"bias\" \n",
    "değeri olduğuna göre bu değerler de bu sayıya toplanmalıdır. O zaman önceki katmandaki nöron sayısı k, bu katmandaki nöron \n",
    "sayısı n ise eğitilebilir parametrelerin sayısı k * n + n tane olur. Aşağıdaki modeli inceleyiniz:\n",
    "\n",
    "model = Sequential(name='Diabetes')\n",
    "\n",
    "model.add(Input((training_dataset_x.shape[1],)))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-1'))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-2'))\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "model.summary()\n",
    "\n",
    "B modelde bir girdi katmanı, iki saklı katman (biz bunlara ara katman da diyeceğiz) bir de çıktı katmanı vardır. \n",
    "summary metodundan elde edilen çıktı şöyledir.\n",
    "\n",
    "Model: \"Diabetes\"\n",
    "┌─────────────────────────────────┬────────────────────────┬───────────────┐\n",
    "│ Layer (type)                    │ Output Shape           │       Param # │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ Hidden-1 (Dense)                │ (None, 16)             │           144 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ Hidden-2 (Dense)                │ (None, 16)             │           272 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ Output (Dense)                  │ (None, 1)              │            17 │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "Total params: 433 (1.69 KB)\n",
    "Trainable params: 433 (1.69 KB)\n",
    "Non-trainable params: 0 (0.00 B)\n",
    "Trainable params: 433 (1.69 KB)\n",
    "Non-trainable params: 0 (0.00 B)\n",
    "\n",
    "Burada ağımızdaki girdi katmanında 8 nöron olduğuna göre ve ilk saklı katmanda da 16 nöron olduğuna göre ilk saklı katmana\n",
    "8 * 16 nöron girmektedir. Öte yandan her nöronun bir tane bias değeri de olduğuna göre ilk katmandaki tahmin ayarlanması \n",
    "gereken parametrelerin (trainable parameters) sayısı 8 * 16 + 16 = 144 tanedir. İkinci saklı katmana 16 nöron dense biçimde\n",
    "bağlanmıştır. O halde ikinci saklı katmandaki ayarlanması gereken parametreler toplamda 16 * 16 + 16 = 272 tanedir. Modelimizin\n",
    "çıktı katmanında 1 nöron vardır. Önceki katmanın 16 çıkışı olduğuna göre bu çıktı katmanında 16 * 1 + 1 = 17 tane ayarlanması\n",
    "gereken parametre vardır. \n",
    "\n",
    "Ağın saklı katmanlarında en çok kullanılan aktivasyon fonksiyonu \"relu\" isimli fonksiyondur. İkili sınıflandırma problemlerinde \n",
    "çıktı katmanı tek nörondan oluşur ve bu katmandaki aktivasyon fonksiyonu \"sigmoid\" fonksiyonu olur. Sigmoid fonksiyonu 0 ile\n",
    "1 arasında bir değer vermektedir. Aktivasyon fonksiyonlarını izleyen paragraflarda ele alacağız.\n",
    "\n",
    "Aşağıdaki örnekte \"dibates\" veri kümesi üzerinde ikili sınıflandırma problemi için bir sinir ağı oluşturulmuştur.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befa117b-780f-484b-a15c-2d162f0e1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Diabetes\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-1 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " Hidden-2 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 433 (1.69 KB)\n",
      "Trainable params: 433 (1.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bu kodda 16+16 seklinde 2 hidden katmandan olusan bir neural network var.Cikistada bir noron output norondan olusan bir katman ornegi var.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si = SimpleImputer(strategy='mean', missing_values=0)\n",
    "df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = si.fit_transform(df[['Glucose', 'BloodPressure', \n",
    "        'SkinThickness', 'Insulin', 'BMI']])\n",
    "\n",
    "#numpya donusum ve datasetlere ayirma kismi\n",
    "#Bu işlemlerin amacı, genellikle makine öğrenmesi modellerinin eğitilmesi için gerekli olan bağımsız değişkenler (X) ve bağımlı değişken (y) ayrıştırmasını yapmaktır. Örneğin, bir doğrusal regresyon problemi için, dataset_x bağımsız değişkenleri (özellikler) içerirken, dataset_y tahmin edilmesi gereken bağımlı değişkeni (hedef değişkeni) içerir\n",
    "dataset = df.to_numpy()\n",
    "dataset_x = dataset[:, :-1]\n",
    "dataset_y = dataset[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential(name='Diabetes') #kerasin model nesnesi yaratilmali\n",
    "\n",
    "#katmanlarin olusturulmasi : noron sayisini belirlerken fazla katman olursa overfitting olabilir dikkatli olmak gerekir.\n",
    "model.add(Input((training_dataset_x.shape[1],)))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-1')) #1 hidden katman nesnesi 16 tane norondan olusuyor demek ki.(1 .katmanda [8*16(W) degeri + 16(Bayes)=144 tane parametre degeri var]\n",
    "model.add(Dense(16, activation='relu', name='Hidden-2')) #(2 .hidden katmanda [16*16(W) degeri + 16(Bayes)=272 tane parametre degeri var]\n",
    "model.add(Dense(1, activation='sigmoid', name='Output')) #(Output katmanda [16*1(W) degeri + 1(Bayes)=17 tane parametre degeri var genelde sigmoid fonksiyonu kullanilir]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2badbc-2d1a-4dd0-836d-1d8355e4792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4) Model oluşturulduktan sonra modelin derlenmesi (compile edilmesi) gerekir. Buradaki \"derleme\" makine diline dönüştürme \n",
    "anlamında bir terim değildir. Eğitim için bazı belirlemelerin yapılması anlamına gelmektedir. Bu işlem Sequential sınıfının \n",
    "compile isimli metoduyla yapılmaktadır. Modelin compile metoduyla derlenmesi sırasında en önemli iki parametre \"loss fonksiyonu\" \n",
    "ve \"optimizasyon algoritması\"dır. Eğitim sırasında ağın ürettiği değerlerin gerçek değerlere yaklaştırılması için w ve bias \n",
    "değerlerinin nasıl güncelleneceğine ilişkin algoritmalara \"optimizasyon algoritmaları\" denilmektedir. Matematiksel optimizasyon \n",
    "işlemlerinde belli bir fonksiyonun minimize edilmesi istenir. İşte minimize edilecek bu fonksiyona da \"loss fonksiyonu\" \n",
    "denilmektedir. Başka bir deyişle optimizasyon algoritması loss fonksiyonun değerini minimize edecek biçimde işlem yapan \n",
    "algoritmadır. Yani optimizasyon algoritması loss fonksiyonunu minimize etmek için yapılan işlemleri temsil etmektedir. \n",
    "\n",
    "Loss fonksiyonları ağın ürettiği değerlerle gerçek değerler arasındaki farklılığı temsil eden fonksiyonlardır. Loss fonksiyonları \n",
    "genel olarak iki girdi alıp bir çıktı vermektedir. Loss fonksiyonunun girdileri gerçek değerler ile ağın ürettiği değerlerdir. \n",
    "Çıktı değeri ise aradaki farklığı belirten bir değerdir. Eğitim sırasında gitgide loss fonksiyonun değerinin düşmesini bekleriz. \n",
    "Tabii loss değerinin düşmesi aslında ağın gerçek değerlere daha yakın değerler üretmesi anlamına gelmektedir. loss fonksiyonları \n",
    "çıktının biçimine yani problemin türüne bağlı olarak seçilmektedir. Örneğin ikili sınıflandırma problemleri için \"binary cross-entropy\", \n",
    "çoklu sınıflandırma problemleri için \"categorical cross-entropy\", lojistik olmayan regresyon problemleri için \"mean squared error\" \n",
    "isimli loss fonksiyonları tercih edilmektedir. \n",
    "\n",
    "Optimizasyon algoritmaları aslında genel yapı olarak birbirlerine benzemektedir. Pek çok problemde bu algoritmaların çoğu benzer \n",
    "performans göstermektedir. En çok kullanılan optimizasyon algoritmaları \"rmsprop\", \"adam\" ve \"sgd\" algoritmalarıdır. Bu algoritmalar \n",
    "\"gradient descent\" denilen genel optimizasyon yöntemini farklı biçimlerde uygulamaktadır. Optimizasyon algoritmaları kursumuzun \n",
    "ilerleyen bölümlerinde ele alınacaktır.\n",
    "\n",
    "compile metodunda optimizasyon algoritması bir yazı olarak ya da tensorflow.keras.optimizers modülündeki sınıflar türünden bir \n",
    "sınıf nesnesi olarak girilebilmektedir. Örneğin:\n",
    "\n",
    "model.compile(optimizer='rmsprop', ...)\n",
    "\n",
    "Örneğin:\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "rmsprop = RMSprop()\n",
    "\n",
    "model.compile(optimizer=rmsprop, ...)\n",
    "\n",
    "Tabii optimizer parametresinin bir sınıf nesnesi olarak girilmesi daha detaylı belirlemelerin yapılmasına olanak sağlamaktadır.\n",
    "Optimizasyon işlemlerinde bazı parametrik değerler vardır. Bunlara makine öğrenmesinde \"üst düzey parametreler (hyper parameters)\"\n",
    "denilmektedir. İşte optimizasyon algoritması bir sınıf nesnesi biçiminde verilirse bu sınıfın __init__ metodunda biz bu üst \n",
    "düzey parametreleri istediğimiz gibi belirleyebiliriz. Eğer optimizasyon algoritması yazısal biçimde verilirse bu üst düzey \n",
    "parametreler default değerlerle kullanılmaktadır. optimzer parametresinin default değeri \"rmsprop\" biçimindedir. Yani biz bu \n",
    "parametre için değer girmezsek default optimazyon algoritması \"rmsprop\" olarak alınacaktır. \n",
    "\n",
    "loss fonksiyonu compile metoduna yine isimsel olarak ya da tensorflow.keras.losses modülündeki sınıflar türünden sınıf nesleri \n",
    "biçiminde ya da doğrudan fonksiyon olarak girilebilmektedir. loss fonksiyonları kısa ya da uzun isim olarak yazısal biçimde \n",
    "kullanılabilmektedir. Tipik loss fonksiyon isimleri şunlardır:\n",
    "\n",
    "'mean_squared_error' ya da 'mse'\n",
    "'mean_absolute_error' ya da 'mae'\n",
    "'mean_absolute_percentage_error' ya da 'mape'\n",
    "'mean_squared_logarithmic_error' ya da 'msle'\n",
    "'categorical_crossentropy'\n",
    "'binary_crossentropy'\n",
    "\n",
    "tensorflow.keras.losses modülündeki fonksiyonlar da kısa ve uzun isimli olarak bulunabilmektedir:\n",
    "\n",
    "tensorflow.keras.losses.mean_squared_error ya da tensorflow.keras.losses.mse\n",
    "tensorflow.keras.losses.mean_absolute_error ya da tensorflow.keras.losses.mae\n",
    "tensorflow.keras.losses.mean_absolute_percentage_error ya da tensorflow.keras.losses.mape\n",
    "tensorflow.keras.losses.mean_squared_logarithmic_error ya da tensorflow.keras.losses.msle\n",
    "tensorflow.keras.losses.categorical_crossentropy\n",
    "tensorflow.keras.losses.binary_crossentropy\n",
    "\n",
    "Bu durumda compile metodu örnek bir biçimde şöyle çağrılabilir:\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "\n",
    "compile metodunun üçüncü önemli parametresi \"metrics\" isimli parametredir. metrics parametresi bir liste ya da demet olarak \n",
    "girilir. metrics parametresi daha sonra açıklanacak olan \"sınama (validation)\" işlemi için kullanılacak fonksiyonları \n",
    "belirtmektedir. Sınamada her zaman zaten bizim loss fonksiyonu olarak belirttiğimiz fonksiyon kullanılmaktadır. metrics \n",
    "parametresinde ilave fonksiyonlar da girilebilmektedir. Örneğin ikili sınıflandırma problemleri için tipik olarak \"binary_accuracy\"\n",
    "denilen metrik fonksiyon, çoklu sınıflandırma için \"categorical_accuracy\" denilen metrik fonksiyon ve lojistik olmayan regresyon\n",
    "problemleri için de \"mean_absolute_error\" isimli metrik fonksiyon sıklıkla kullanılmaktadır. Örneğin ikili sınıflandırma \n",
    "problemi için biz eğitim sırasında \"loss\" değerinin yanı sıra \"binary_accuracy\" değerini de elde etmek isteyelim. Bu durumda \n",
    "compile metodunu şöyle çağırmalıyız:\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "5) Model derlenip çeşitli belirlemeler yapıldıktan sonra artık gerçekten eğitim aşamasına geçilir. Eğitim süreci Sequential \n",
    "sınıfının fit metoduyle yapılmaktadır. fit metodunun en önemli parametresi ilk iki parametre olan x ve y veri kümeleridir. \n",
    "Biz burada training_dataset_x ve training_dataset_y verilerini fit metodunun ilk iki paramtresine geçirmeliyiz. \n",
    "\n",
    "fit metodunun önemli bir parametresi batch_size isimli parametredir. Eğitim işlemi aslında satır satır değil batch batch \n",
    "yapılmaktadır. batch bir grup satıra denilmektedir. Yani ağa bir grup satır girdi olarak verilir. Ağdan bir grup çıktı elde \n",
    "edilir. Bu bir grup çıktı ile bu çıktıların gerçek değerleri loss fonksiyonuna sokulur ve optimizasyon algoritması çalıştırılarak \n",
    "w ve bias değerleri güncellenir. Yani optimizasyon algoritması her batch işlemden sonra devreye sokulmaktadır. Batch büyüklüğü \n",
    "fit metodunda batch_size parametresiyle belirtilmektedir. Bu değer girilmezse batch_size 32 olarak alınmaktadır. 32 değeri pek \n",
    "çok uygulama için uygun bir değerdir. Optimizasyon işleminin satır satır yapılması yerine batch batch yapılmasının iki önemli \n",
    "nedeni vardır: Birincisi işlem miktarının azaltılması, dolayısıyla eğitim süresinin kısaltılmasıdır. İkincisi ise \"overfitting\" \n",
    "denilen olumsuz durum için bir önlem oluşturmasıdır. Overfitting hakkında ileride açıklama yapılacaktır. \n",
    "\n",
    "fit metodunun diğer önemli parametresi de \"epochs\" isimli parametredir. eğitim veri kümesinin eğitim sırasında yeniden \n",
    "eğitimde kullanılmasına \"epoch\" işlemi denilmektedir. Örneğin elimizde 1000 satırlık bir eğitim veri kümesi olsun. batch_size \n",
    "parametresinin de 20 olduğunu varsayalım. Bu durumda bu eğitim veri kümesi 1000 / 20 = 50 batch işleminde bitecektir. Yani \n",
    "model parametreleri 50 kere ayarlanacaktır. Pek çok durumda eğitim veri kümesinin bir kez işleme sokulması model parametrelerinin\n",
    "iyi bir biçimde konumlandırılması için yetersiz kalmaktadır. İşte eğitim veri kümesinin birden fazla kez yani fit metodundaki \n",
    "epochs sayısı kadar yeniden eğitimde kullanılması yoluna gidilmektedir. Pekiyi epochs değeri ne olmalıdır? Aslında bunu uygulamacı \n",
    "belirler. Az sayıda epoch model parametrelerini yeterince iyi konumlandıramayabilir. Çok fazla sayıda epoch \"overfitting\" \n",
    "denilen olumsuz duruma zemin hazırlayabilir. Ayrıca çok fazla epoch eğitim zamanını da uzatmaktadır. Uygulamacı epoch'lar \n",
    "sırasında modelin davranışına bakabilir ve uygun epoch sayısında işlemi kesebilir. Eğitim sırasında Keras bizim belirlediğimiz \n",
    "fonksiyonları çağırabilmektedir. Buna Keras'ın \"callback\" mekanizması denilmektedir. Uygulamacı bu yolla model belli bir \n",
    "duruma geldiğinde eğitim işlemini kesebilir. Ya da uygulamacı eğer eğitim çok uzamayacaksa yüksek bir epoch ile eğitimini \n",
    "yapabilir. İşlemler bitince epoch'lardaki performansa bakabilir. Olması gerekn epoch değerini kestirebilir. Sonra modeli \n",
    "yeniden bu sayıda epoch ile eğitir. fit metodunun shuffle parametresi her epoch'tan sonra eğitim veri kümesinin karıştırılıp \n",
    "karıştırılmayacağını belirtmektedir. Bu parametre default olarak True biçimdedir. Yani eğitim sırasında her epoch'ta eğitim \n",
    "veri kümesi karıştırılmaktadır. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7275076-fc02-4529-b88f-f55f77c04557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelin fit metodu ile eğitilmesi sırasında \"sınama (validation)\" denilen önemli bir kavram daha devreye girmektedir. Sınama\n",
    "işlemi test işlemine benzemektedir. Ancak test işlemi tüm model eğitildikten sonra yapılırken sınama işlemi her epoch'tan \n",
    "sonra modelin eğitim sürecinde yapılmaktadır. Başka bir deyişle sınama işlemi model eğitilirken yapılan test işlemidir. \n",
    "Epoch'lar sırasında modelin performansı hakkında bilgi edinebilmek için sınama işlemi yapılmaktadır. Sınamanın yapılması \n",
    "için fit metodunun validation_split parametresinin 0 ile 1 arasında oransal bir değer olarak girilmesi gerekir. Bu oransal \n",
    "değer eğitim veri kümesinin yüzde kaçının sınama için kullanılacağını belirtmektedir. Örneğin validation_split=0.2 eğitim \n",
    "veri kümesinin %20'sinin sınama için kullanılacağını belirtmektedir. fit metodu işin başında eğitim veri kümesini eğitimde \n",
    "kullanılacak kısım ile sınamada kullanılacak kısım biçiminde ikiye ayırmaktadır. Sonra her epoch'ta yalnızca eğitimde \n",
    "kullanılacak kümeyi karıştırmaktadır. Sınama işlemi aynı kümeyle her epoch sonrasında karıştırılmadan yapılmaktadır. fit \n",
    "metodunda ayrıca bir de validation_data isimli bir parametre vardır. Bu parametre sınama verilerini girmek için kullanılmaktadır. \n",
    "Bazen programcı sınama verilerinin eğitim veri kümesinden çekilip alınmasını istemez. Onu ayrıca fit metoduna vermek isteyebilir. \n",
    "Tabii validation_data parametresi girildiyse artık validation_split parametresinin bir anlamı yoktur. Bu parametre girilse bile \n",
    "artık fonksiyon tarafından dikkate alınmaz. Örneğin:\n",
    "\n",
    "model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)\n",
    "\n",
    "validation_split parametresinin default değerinin 0 olduğuna dikkat ediniz. validation_split değerinin 0 olması epoch'lar \n",
    "sonrasında sınama işleminin yapılmayacağı anlamına gelmektedir. \n",
    "\n",
    "Pekiyi modelin fit metodunda her epoch'tan sonra sınama işleminde hangi ölçümler ekrana yazdırılacaktır? İşte compile metodunda \n",
    "belirtilen ve ismine metrik fonksiyonlar denilen fonksiyonlar her epoch işlemi sonucunda ekrana yazdırılmaktadır. Her epoch \n",
    "sonucunda fit metodu şu değerleri yazdırmaktadır:\n",
    "\n",
    "- Epoch sonrasında elde edilen loss değerlerinin ortalaması\n",
    "- Epoch sonrasında eğitim veri kümesinin kendisi için elde edilen metrik değerlerin ortalaması\n",
    "- Epoch sonrasında sınama için kullanılan sınama verilerinden elde edilen loss değerlerinin ortalaması\n",
    "- Epoch sonrasında sınama için kullanılan sınama verilerinden elde edilen metrik değerlerin ortalaması\n",
    "\n",
    "Bir epoch işleminin batch batch yapıldığını anımsayınız. Budurumda epoch sonrasında fit tarafından ekrana yazdırılan değerler\n",
    "bu batch işlemlerden elde edilen ortalama değerlerdir. Yani çrneğin her batch işleminden bir loss değeri elde edilir. Sonra \n",
    "bu loss değerlerinin ortalaması hesap edilerek yazdırılır. Eğitim veri kümesindeki değerler ile sınama veri kümesinden elde \n",
    "edilen değerler birbirine karışmasın diye fit metodu sınama verilerinden elde edilen değerlerin başına \"val_\" öneki getirmektedir. \n",
    "Örneğin biz ikili sınıflandırma problemi üzerinde çalışıyor olalım ve metrik fonksion olarak \"binary_accuracy\" kullanmış olalım. \n",
    "fit metodu her epoch sonrasında şu değerleri ekrana yazdıracaktır:\n",
    "\n",
    "loss (eğitim veri kümesinden elde edilen ortalama loss değeri)\n",
    "binary_accuracy (eğitim veri kümesinden elde edilen ortalama metrik değer)\n",
    "val_loss (sınama veri kümesinden elde edilen ortalama loss değeri)\n",
    "val_binary_accuracy (sınama veri kümesinden elde edilen ortalama metrik değer)\n",
    "\n",
    "Tabii compile metodunda birden fazla metirk değer de belirtilmiş olabilir. Bu durumda fit tüm bu metrik değerlerin ortalamasını\n",
    "ekrana yazdıracaktır. fit tarafından ekrana yazdırılan örnek bir çıktı şöyle olabilir:\n",
    "\n",
    "....\n",
    "Epoch 91/100\n",
    "16/16 [==============================] - 0s 3ms/step - loss: 0.5536 - binary_accuracy: 0.7230 - val_loss: 0.5520 - \n",
    "val_binary_accuracy: 0.7480\n",
    "Epoch 92/100\n",
    "16/16 [==============================] - 0s 3ms/step - loss: 0.5392 - binary_accuracy: 0.7251 - val_loss: 0.5588 - \n",
    "val_binary_accuracy: 0.7805\n",
    "Epoch 93/100\n",
    "16/16 [==============================] - 0s 3ms/step - loss: 0.5539 - binary_accuracy: 0.7088 - val_loss: 0.5666 - \n",
    "val_binary_accuracy: 0.8049\n",
    "...\n",
    "\n",
    "Burada loss değeri epoch sonrasında eğitim verilerinden elde edilen ortalama loss değerini, val_loss değeri epoch sonrasında \n",
    "sınama verilerinden elde edilen ortalama loss değerini, binary_accuracy epoch sonrasında eğitim verilerinden elde edilen\n",
    "ortalama isabet yüzdesini ve val_binary_accuracy ise epoch sonrasında sınama verilerinden elde edilen ortalama isabet \n",
    "yüzdesini belirtmektedir.\n",
    "\n",
    "Eğitim sırasında eğitim veri kümesindeki başarının sınama veri kümesinde görülmemesi eğitimin kötü bir yöne gittiğine işaret \n",
    "etmektedir. Örneğin ikili sınıflandırma probleminde epoch sonucunda eğitim veri kümesindeki binary_accuracy değerinin %99 \n",
    "olduğunu ancak val_binary_accuracy değerinin %65 olduğunu düşünelim. Bunun anlamı ne olabilir? Bu durum aslında epoch'lar \n",
    "sırasında modelin bir şeyler öğrendiği ama bizim istediğimiz şeyleri öğrenemediği anlamına gelmektedir. Çünkü eğitim veri \n",
    "kümesini epoch'larla sürekli bir biçimde gözden geçiren model artık onu ezberlemiştir. Ancak o başarıyı eğitimden bağımsız \n",
    "bir veri kümesinde gösterememektedir. İşte bu olguya \"overfitting\" denilmektedir. Yanlış bir şeyin öğrenilmesi bir şeyin \n",
    "öğrenilememesi kadar kötü bir durumdur. Overfitting oluşumunun çeşitli nedenleri vardır. Ancak overfitting epoch'lar \n",
    "dolayısıyla oluşuyorsa epoch'ları uygun bir noktada kesmek gerekir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a86e18-60c7-41f0-be65-f0302259845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6) fit işleminden sonra artık model eğitilmiştir. Onun test veri kümesiyle test edilmesi gerekir. Bu işlem Sequential sınıfının \n",
    "evaluate isimli metodu ile yapılmaktadır. evaluate metodunun ilk iki parametresi test_dataset_x ve test_dataset_y değerlerini \n",
    "almaktadır. Diğer bir parametresi yine batch_size parametresidir. Buradaki bacth_size eğitim işlemi yapılırken fit metodunda \n",
    "kullanılan batch_size ile benzer anlamdadır ancak işlevleri farklıdır. Model test edilirken test işlemi de birer birer değil \n",
    "batch batch yapılabilmektedir. Ancak bu batch'ler arasında herhangi bir şey yapılmamaktadır. (Eğitim sırasındaki batch işlemleri \n",
    "sonrasında ağ parametrelerinin ayarlandığını anımsayınız. Test işlemi sırasında böyle bir ayarlama yapılmamaktadır.) Buradaki \n",
    "batch değeri yalnızca işlemlerin hızlı yapılması üzerinde etkili olmaktadır. Yine batch_size parametresi girilmezse default \n",
    "32 alınmaktadır. evaluate metodu bir liste geri döndürmektedir. Listenin ilk elemanı test veri kümesinden elde edilen loss \n",
    "fonksiyonunun değeridir. Diğer elemanları da sırasıyla metrik olarak verilen fonksiyonların değerleridir. Örneğin:\n",
    "\n",
    "eval_result = model.evaluate(test_dataset_x, test_dataset_y)\n",
    "\n",
    "Aslında eval_result listesinin elemanlarının hangi anlamlara geldiğini daha iyi ifade edebilmek için Sequential sınıfında \n",
    "metrics_names isimli bir örnek özniteliği (instance attribute) bulundurulmuştur. Bu metrics_names listesindeki isimler bire \n",
    "bir evalute metodunun geri döndürdüğü liste elemanları ile örtüşmektedir. Bu durumda evaluate metodunun geri döndürdüğü listeyi \n",
    "aşağıdaki gibi de yazdırabiliriz:\n",
    "\n",
    "for i in range(len(eval_result)):\n",
    "    print(f'{model.metrics_names[i]}: {eval_result[i]}')\n",
    "\n",
    "Aynı şeyi built-in zip fonksiyonuyla da şöyle yapabilirdik:\n",
    "\n",
    "for name, value in zip(model.metrics_names, eval_result):\n",
    "    print(f'{name}: {value}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb59ff5-2705-4a01-a3b1-90355f38688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7) Artık model test de edilmiştir. Şimdi sıra \"kestirim (prediction)\" yapmaya gelmiştir. Kestirim işlemi için Sequential\n",
    "sınıfının predict metodu kullanılır. Biz bu metoda girdi katmanına uygulanacak özelliklerin (yani satırların) değerlerini \n",
    "veriririz. predict metodu da bize çıktı katmanındaki nöronların değerlerini verir. predict metoduna biz her zaman iki boyutlu \n",
    "bir NumPy dizisi vermeliyiz. Çünkü predict metodu tek hamlede birden çok satır için kestirim yapabilmektedir. Biz predict \n",
    "metoduna bir satır verecek olsak bile onu iki boyutlu bir matris biçiminde vermeliyiz. Örneğin:\n",
    "\n",
    "import numpy as np\n",
    "ır \n",
    "predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],\n",
    "                        [4, 111, 79, 47, 207, 37.1, 1.39, 56],\n",
    "                        [3, 190, 65, 25, 130, 34, 0.271, 26],\n",
    "                        [8, 176, 90, 34, 300, 50.7, 0.467, 58],\n",
    "                        [7, 106, 92, 18, 200, 35, 0.300, 48]])\n",
    "\n",
    "predict_result = model.predict(predict_data)\n",
    "print(predict_result)\n",
    "\n",
    "predict metodu bize tahmin edilen değerleri iki boyutlu bir NumPy dizisi biçiminde vermektedir. Bunun nedeni aslında ağın \n",
    "birden fazla çıktısının olabilmesidir. Örneğin ağın bir çıktısı varsa bu durumda predict metodu bize \"n tane satırdan 1 tane\n",
    "sütundan\" oluşan bir matris, ağın iki çıktısı varsa \"n tane satırdan 2 iki tane sütundan oluşan bir matris verecektir. O halde \n",
    "örneğin çıktı olarak tek nöronun bulunduğu bir ağda (\"diabetes\" örneğindeki gibi) biz kestirim değerlerini şöyle yazdırabiliriz:\n",
    "\n",
    "for i in range(len(predict_result)):\n",
    "    print(predict_result[i, 0])\n",
    "\n",
    "Ya da şöyle yazdırabiliriz:\n",
    "\n",
    "for result in predict_result[:, 0]:\n",
    "    print(result)\n",
    "\n",
    "Tabii iki boyutlu diziyi Numpy'ın flatten metoduyla ya da ravel metoduyla tek boyutlu hale getirerek de yazırma işlemini \n",
    "yapabilirdik:\n",
    "\n",
    "for val in predict_result.flatten():\n",
    "    print(val)\n",
    "\n",
    "predict metodu bize ağın çıktı değerini vermektedir. Yukarıdaki \"diabetes.csv\" örneğimizde ağın çıktı katmanındaki aktivasyon \n",
    "fonksiyonunun \"sigmoid\" olduğunu anımsayınız. Sigmoid fonksiyonu 0 ile 1 arasında bir değer vermektedir. O halde biz ağın \n",
    "çıktısındaki değer 0.5'ten büyükse ilgili kişinin şeker hastası olduğu (çünkü 1'e daha yakındır), 0.5'ten küçükse o kişinin \n",
    "şeker hastası olmadığı (çünkü 0'a daha yakındır) sonucunu çıkartabiliriz. Tabii değer ne kadar 1'e yakınsa kişininin şeker \n",
    "hastası olma olasılığı, değer ne kadar 0'a yakınsa kişinin şeker hastası olmama olasılığı o kadar yüksek olacaktır. O halde \n",
    "sigmoid fonksiyonun çıktısının bir olasılık belirttiğini söyleyebiliriz. Bu durumda kişinin şeker hastası olup olmadığı ağın \n",
    "çıktı değerinin 0.5'ten büyük olup olmamasıyla kesitirilebilir:\n",
    "\n",
    "for result in predict_result[:, 0]:\n",
    "    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ee2cf9-1773-4519-9d76-c46bd5675b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Diabetes\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-1 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " Hidden-2 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 433 (1.69 KB)\n",
      "Trainable params: 433 (1.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 27ms/step - loss: 18.3540 - binary_accuracy: 0.6395 - val_loss: 9.4262 - val_binary_accuracy: 0.6423\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.0764 - binary_accuracy: 0.5295 - val_loss: 2.2407 - val_binary_accuracy: 0.5122\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.4573 - binary_accuracy: 0.4562 - val_loss: 1.5210 - val_binary_accuracy: 0.4797\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.4301 - binary_accuracy: 0.4623 - val_loss: 1.0342 - val_binary_accuracy: 0.6423\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1523 - binary_accuracy: 0.5112 - val_loss: 1.9286 - val_binary_accuracy: 0.3984\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9890 - binary_accuracy: 0.5703 - val_loss: 0.9683 - val_binary_accuracy: 0.5610\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9117 - binary_accuracy: 0.5743 - val_loss: 0.8362 - val_binary_accuracy: 0.5447\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8495 - binary_accuracy: 0.5988 - val_loss: 0.7899 - val_binary_accuracy: 0.5935\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7790 - binary_accuracy: 0.6477 - val_loss: 0.9433 - val_binary_accuracy: 0.6504\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7182 - binary_accuracy: 0.6436 - val_loss: 1.3386 - val_binary_accuracy: 0.4309\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7743 - binary_accuracy: 0.6191 - val_loss: 1.0370 - val_binary_accuracy: 0.6585\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6798 - binary_accuracy: 0.6640 - val_loss: 1.3924 - val_binary_accuracy: 0.4472\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7210 - binary_accuracy: 0.6477 - val_loss: 0.7323 - val_binary_accuracy: 0.6341\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7029 - binary_accuracy: 0.6497 - val_loss: 0.9977 - val_binary_accuracy: 0.6423\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6494 - binary_accuracy: 0.6782 - val_loss: 0.8714 - val_binary_accuracy: 0.5528\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7383 - binary_accuracy: 0.6273 - val_loss: 1.0278 - val_binary_accuracy: 0.5203\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6720 - binary_accuracy: 0.6864 - val_loss: 0.7410 - val_binary_accuracy: 0.6585\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6826 - binary_accuracy: 0.6599 - val_loss: 0.6956 - val_binary_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7030 - binary_accuracy: 0.6354 - val_loss: 0.6821 - val_binary_accuracy: 0.6423\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6286 - binary_accuracy: 0.6925 - val_loss: 0.8107 - val_binary_accuracy: 0.6585\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6361 - binary_accuracy: 0.6701 - val_loss: 0.8561 - val_binary_accuracy: 0.5772\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6356 - binary_accuracy: 0.6864 - val_loss: 0.6869 - val_binary_accuracy: 0.6585\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6251 - binary_accuracy: 0.7026 - val_loss: 0.7031 - val_binary_accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6532 - binary_accuracy: 0.6864 - val_loss: 0.7098 - val_binary_accuracy: 0.6748\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6376 - binary_accuracy: 0.6802 - val_loss: 1.1838 - val_binary_accuracy: 0.4634\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6251 - binary_accuracy: 0.6843 - val_loss: 0.7664 - val_binary_accuracy: 0.6260\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.6864 - val_loss: 0.9710 - val_binary_accuracy: 0.6504\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6454 - binary_accuracy: 0.7026 - val_loss: 0.6719 - val_binary_accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6138 - binary_accuracy: 0.7026 - val_loss: 0.6674 - val_binary_accuracy: 0.6829\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5873 - binary_accuracy: 0.7067 - val_loss: 0.6522 - val_binary_accuracy: 0.6911\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6319 - binary_accuracy: 0.6782 - val_loss: 0.9935 - val_binary_accuracy: 0.6423\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6285 - binary_accuracy: 0.7026 - val_loss: 0.7606 - val_binary_accuracy: 0.6423\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6326 - binary_accuracy: 0.6802 - val_loss: 0.9340 - val_binary_accuracy: 0.6423\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6180 - binary_accuracy: 0.6680 - val_loss: 0.8439 - val_binary_accuracy: 0.6098\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5952 - binary_accuracy: 0.6945 - val_loss: 1.2401 - val_binary_accuracy: 0.4390\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6229 - binary_accuracy: 0.6843 - val_loss: 0.7555 - val_binary_accuracy: 0.6423\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5945 - binary_accuracy: 0.7088 - val_loss: 0.8085 - val_binary_accuracy: 0.6098\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5598 - binary_accuracy: 0.7230 - val_loss: 0.7065 - val_binary_accuracy: 0.6829\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5904 - binary_accuracy: 0.7088 - val_loss: 1.2777 - val_binary_accuracy: 0.4797\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6238 - binary_accuracy: 0.6802 - val_loss: 0.6862 - val_binary_accuracy: 0.6911\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5797 - binary_accuracy: 0.6965 - val_loss: 0.8608 - val_binary_accuracy: 0.5610\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5609 - binary_accuracy: 0.7230 - val_loss: 0.6806 - val_binary_accuracy: 0.7154\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5783 - binary_accuracy: 0.7067 - val_loss: 0.6665 - val_binary_accuracy: 0.6911\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5876 - binary_accuracy: 0.7067 - val_loss: 0.6873 - val_binary_accuracy: 0.6748\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5630 - binary_accuracy: 0.7251 - val_loss: 0.8053 - val_binary_accuracy: 0.6423\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6001 - binary_accuracy: 0.7128 - val_loss: 0.9971 - val_binary_accuracy: 0.6504\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5951 - binary_accuracy: 0.7006 - val_loss: 0.6824 - val_binary_accuracy: 0.7073\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5679 - binary_accuracy: 0.7169 - val_loss: 1.9878 - val_binary_accuracy: 0.3984\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5955 - binary_accuracy: 0.7006 - val_loss: 0.8051 - val_binary_accuracy: 0.6179\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5855 - binary_accuracy: 0.7128 - val_loss: 0.6415 - val_binary_accuracy: 0.6829\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6059 - binary_accuracy: 0.6864 - val_loss: 0.8676 - val_binary_accuracy: 0.6260\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6212 - binary_accuracy: 0.6986 - val_loss: 0.7274 - val_binary_accuracy: 0.6748\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6053 - binary_accuracy: 0.7210 - val_loss: 0.7059 - val_binary_accuracy: 0.6748\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5556 - binary_accuracy: 0.7128 - val_loss: 0.6806 - val_binary_accuracy: 0.6911\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5971 - binary_accuracy: 0.6986 - val_loss: 1.3087 - val_binary_accuracy: 0.4390\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6090 - binary_accuracy: 0.6904 - val_loss: 0.6128 - val_binary_accuracy: 0.6585\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5687 - binary_accuracy: 0.7026 - val_loss: 0.7497 - val_binary_accuracy: 0.6504\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5706 - binary_accuracy: 0.7332 - val_loss: 0.7056 - val_binary_accuracy: 0.6829\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6465 - binary_accuracy: 0.6701 - val_loss: 0.6953 - val_binary_accuracy: 0.6829\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5366 - binary_accuracy: 0.7393 - val_loss: 0.8822 - val_binary_accuracy: 0.6504\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6236 - binary_accuracy: 0.7108 - val_loss: 0.6489 - val_binary_accuracy: 0.6911\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5234 - binary_accuracy: 0.7291 - val_loss: 0.9156 - val_binary_accuracy: 0.5041\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5596 - binary_accuracy: 0.7169 - val_loss: 1.0505 - val_binary_accuracy: 0.6504\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5774 - binary_accuracy: 0.7128 - val_loss: 0.8342 - val_binary_accuracy: 0.6423\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5653 - binary_accuracy: 0.7251 - val_loss: 0.8502 - val_binary_accuracy: 0.5528\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5886 - binary_accuracy: 0.6904 - val_loss: 0.6530 - val_binary_accuracy: 0.7154\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5598 - binary_accuracy: 0.7088 - val_loss: 1.2062 - val_binary_accuracy: 0.6504\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6333 - binary_accuracy: 0.7026 - val_loss: 0.8176 - val_binary_accuracy: 0.6423\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6104 - binary_accuracy: 0.7047 - val_loss: 0.7054 - val_binary_accuracy: 0.6667\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5728 - binary_accuracy: 0.7088 - val_loss: 0.6694 - val_binary_accuracy: 0.6992\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6095 - binary_accuracy: 0.6986 - val_loss: 0.6243 - val_binary_accuracy: 0.7154\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5472 - binary_accuracy: 0.7149 - val_loss: 0.6474 - val_binary_accuracy: 0.6829\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6142 - binary_accuracy: 0.7128 - val_loss: 0.6676 - val_binary_accuracy: 0.6992\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5966 - binary_accuracy: 0.6965 - val_loss: 0.6370 - val_binary_accuracy: 0.7073\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6029 - binary_accuracy: 0.6965 - val_loss: 0.7232 - val_binary_accuracy: 0.6829\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5440 - binary_accuracy: 0.7210 - val_loss: 1.0943 - val_binary_accuracy: 0.6504\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5654 - binary_accuracy: 0.7312 - val_loss: 0.6806 - val_binary_accuracy: 0.6829\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6145 - binary_accuracy: 0.7128 - val_loss: 0.7668 - val_binary_accuracy: 0.6504\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5877 - binary_accuracy: 0.6904 - val_loss: 0.6598 - val_binary_accuracy: 0.6911\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5979 - binary_accuracy: 0.6843 - val_loss: 0.7164 - val_binary_accuracy: 0.6748\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5750 - binary_accuracy: 0.6986 - val_loss: 0.7218 - val_binary_accuracy: 0.6911\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5891 - binary_accuracy: 0.6782 - val_loss: 1.0175 - val_binary_accuracy: 0.5041\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5882 - binary_accuracy: 0.7047 - val_loss: 0.9473 - val_binary_accuracy: 0.5366\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5579 - binary_accuracy: 0.7210 - val_loss: 1.2346 - val_binary_accuracy: 0.6504\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5636 - binary_accuracy: 0.7332 - val_loss: 0.7631 - val_binary_accuracy: 0.6098\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6238 - binary_accuracy: 0.6619 - val_loss: 0.8093 - val_binary_accuracy: 0.6016\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5585 - binary_accuracy: 0.7108 - val_loss: 0.7658 - val_binary_accuracy: 0.6748\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5746 - binary_accuracy: 0.7088 - val_loss: 0.7159 - val_binary_accuracy: 0.6585\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5770 - binary_accuracy: 0.6986 - val_loss: 0.6330 - val_binary_accuracy: 0.7073\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.7128 - val_loss: 0.7184 - val_binary_accuracy: 0.6748\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5407 - binary_accuracy: 0.7332 - val_loss: 0.9677 - val_binary_accuracy: 0.5203\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6098 - binary_accuracy: 0.6864 - val_loss: 0.7549 - val_binary_accuracy: 0.6911\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5504 - binary_accuracy: 0.7230 - val_loss: 0.6536 - val_binary_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5849 - binary_accuracy: 0.7189 - val_loss: 1.4802 - val_binary_accuracy: 0.4228\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5810 - binary_accuracy: 0.7047 - val_loss: 1.1080 - val_binary_accuracy: 0.6504\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5815 - binary_accuracy: 0.7169 - val_loss: 0.7782 - val_binary_accuracy: 0.5935\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5480 - binary_accuracy: 0.7169 - val_loss: 0.7851 - val_binary_accuracy: 0.6098\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5684 - binary_accuracy: 0.7006 - val_loss: 0.8312 - val_binary_accuracy: 0.5854\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5758 - binary_accuracy: 0.7026 - val_loss: 0.6753 - val_binary_accuracy: 0.7073\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5348 - binary_accuracy: 0.7434 - val_loss: 1.3354 - val_binary_accuracy: 0.4228\n",
      "5/5 [==============================] - 0s 651us/step - loss: 1.1402 - binary_accuracy: 0.4221\n",
      "loss: 1.1402274370193481\n",
      "binary_accuracy: 0.4220779240131378\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "[[0.83617586]\n",
      " [0.8768331 ]\n",
      " [0.9045127 ]\n",
      " [0.95299864]\n",
      " [0.7326825 ]]\n",
      "Şeker hastası\n",
      "Şeker hastası\n",
      "Şeker hastası\n",
      "Şeker hastası\n",
      "Şeker hastası\n"
     ]
    }
   ],
   "source": [
    "#Aşağıdaki örnekte \"diabetes.csv\" dosyası üzerinde yukarıda belirtilen tüm adımlar uygulanmıştır.\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(strategy='mean', missing_values=0)\n",
    "\n",
    "df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = si.fit_transform(df[['Glucose', 'BloodPressure', \n",
    "        'SkinThickness', 'Insulin', 'BMI']])\n",
    "\n",
    "dataset = df.to_numpy()\n",
    "\n",
    "dataset_x = dataset[:, :-1]\n",
    "dataset_y = dataset[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential(name='Diabetes')\n",
    "\n",
    "model.add(Input((training_dataset_x.shape[1],)))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-1'))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-2'))\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)\n",
    "eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)\n",
    "\n",
    "for i in range(len(eval_result)):\n",
    "    print(f'{model.metrics_names[i]}: {eval_result[i]}')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],\n",
    "                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],\n",
    "                            [3, 190, 65, 25, 130, 34, 0.271, 26],\n",
    "                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],\n",
    "                            [7, 106, 92, 18, 200, 35, 0.300, 48]])\n",
    "\n",
    "predict_result = model.predict(predict_dataset)\n",
    "print(predict_result)\n",
    "\n",
    "for result in predict_result[:, 0]:\n",
    "    print('Şeker hastası' if result > 0.5 else 'Şeker Hastası Değil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6f0e1-6b3d-4031-b602-1abb7ce2d5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
